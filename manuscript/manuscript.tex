\documentclass[onecolumn]{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{url}
\usepackage[round]{natbib}
\usepackage[utf8]{inputenc}
% To make two column figures appear in the correct order
\usepackage{fixltx2e}
% To make annotations on the sides
\usepackage{todonotes}
% To generate dummy text
\usepackage{lipsum}
% Metadata for the generated PDF
\usepackage[pdftex,colorlinks=true]{hyperref}
\hypersetup{
    allcolors=blue,
}

\textwidth 6.2in
\textheight 9in
\topmargin -.5in
\oddsidemargin 0in
\evensidemargin 0in


\begin{document}

\title{Implicit models in geophysical inverse problems}
\author{
    Leonardo Uieda$^{1}$
    and
    Vanderlei C. Oliveira Jr.$^{2}$
    \\\\
    {\small $^1$Universidade do Estado do Rio de Janeiro, Rio de Janeiro, Brazil.
        e-mail: leouieda@gmail.com}
    \\
    {\small $^2$Observat√≥rio Nacional, Rio de Janeiro, Brazil.
        e-mail: vandscoelho@gmail.com}
}


\maketitle


\begin{abstract}
    \lipsum[1]
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Cite things using \citet{tikhonov1977} or \citep{tikhonov1977}.

%\begin{figure}
    %\centering
    %\includegraphics[]{figures/example}
    %\caption{
        %Example image.
    %}
    %\label{fig:meh}
%\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parametric models}

Let $\mathbf{y}$ be an $N$-dimensional vector of observed data.
In traditional geophysical inversion theory,
we assume that there is a functional relation between
an $M$-dimensional vector of model parameters $\mathbf{p}$
and a vector of predicted data $\mathbf{d}$,

\begin{equation}
    \mathbf{d} = \mathbf{f}(\mathbf{p}),
    \label{eq:parametric}
\end{equation}

\noindent
in which $\mathbf{f}$ is a vector representing the $L = N$ functions
$f_1, f_2, \ldots, f_L$.
Equation~\ref{eq:parametric} represents a parametric model.

We want to find a vector $\mathbf{p}$ that minimizes the misfit between the
observed data ($\mathbf{y}$) and the data predicted by the model
($\mathbf{d}$).
We also want the estimated parameters to obey certain constraints.
These conditions can be imposed using Tikhonov regularization
\citep{tikhonov1977} by minimizing the objective function

\begin{equation}
    \phi(\mathbf{p}) = \mathbf{r}^T\mathbf{Q}\mathbf{r}
                       + \mu \mathbf{p}^T\mathbf{W}\mathbf{p},
    \label{eq:parametric-goal}
\end{equation}

\noindent
where $\mathbf{r} = \mathbf{y} - \mathbf{d}$ is the residuals vector,
$\mu$ is the regularization parameter, and $\mathbf{Q}$ and $\mathbf{W}$ are
weight matrices.

Equation~\ref{eq:parametric-goal} can be minimized using, for example, the
Gauss-Newton method.
We update an initial parameter vector $\mathbf{p}^0$ by $\mathbf{p}^1 =
\mathbf{p}^0 + \mathbf{\Delta p}^0$, repeating this operation until convergence
is reached.
At the $k$th iteration, $\mathbf{\Delta p}^k$ is given by

\begin{equation}
    \left[ {\mathbf{A}^k}^T \mathbf{Q} \mathbf{A}^k + \mu\mathbf{W} \right]
    \mathbf{\Delta p}^k =
        {\mathbf{A}^k}^T\mathbf{Q}\mathbf{r}^k
        - \mu\mathbf{W}\mathbf{p}^k.
    \label{eq:gauss-newton}
\end{equation}

\noindent
The $N \times M$ dimensional matrix $\mathbf{A}$ is the Jacobian matrix of
$\mathbf{f}$ whose elements are

\begin{equation}
    A_{ij} = \dfrac{\partial f_i}{\partial p_j}.
    \label{eq:jacobian-params}
\end{equation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Classical theory of implicit models}

\todo{\citet{menke2012} briefly described the problem with a different
formulation.}

We will follow the derivations in \citet{vanicek1986}.

Note that the observed data $\mathbf{l}$
are only included
as the initial estimate
for the predicted data $\mathbf{d}^0$.
So after the first iteration
the observed data are not present
in any of the equations.

Definitions:
$\mathbf{d}$ is the $N$-dimensional vector of predicted data,
$\mathbf{l}$ is the $N$-dimensional vector of observed data,
$\mathbf{p}$ is the $M$-dimensional vector of model parameters,

Sometimes it is impossible to separate the data from the model parameters.
In such cases,
we must use an implicit mathematical model

\begin{equation}
    \mathbf{f}(\mathbf{p}, \mathbf{d}) = \mathbf{0}.
    \label{eq:implicit}
\end{equation}

$\mathbf{f}$ is a vector of $L$ equations.

We must estimate both the model parameters and the predicted data.

First, we linearize Equation~\ref{eq:implicit} by expanding it to first order
in a Taylor series around $\mathbf{p}^0$ and $\mathbf{d}^0$

\begin{equation}
    \mathbf{f}^0 + \mathbf{A}\mathbf{\delta} + \mathbf{B}\mathbf{v} =
    \mathbf{0},
    \label{eq:implicit-taylor}
\end{equation}

\noindent
in which
$\mathbf{f}^0 = \mathbf{f}(\mathbf{p}^0, \mathbf{d}^0)$,
$\mathbf{\delta} = \mathbf{p} - \mathbf{p}^0$,
$\mathbf{v} = \mathbf{d} - \mathbf{d}^0$,
$\mathbf{A}$ is
the $L \times M$ dimensional Jacobian matrix of $\mathbf{f}$
with respect to $\mathbf{p}$,
and
$\mathbf{B}$ is
the $L \times N$ dimensional Jacobian matrix of $\mathbf{f}$
with respect to $\mathbf{d}$.
The element $A_{ij}$ of matrix $\mathbf{A}$ is

\begin{equation}
    A_{ij} = \dfrac{\partial f_i}{\partial p_j},
    \label{eq:A}
\end{equation}

\noindent
and the element $B_{ij}$ of matrix $\mathbf{B}$ is

\begin{equation}
    B_{ij} = \dfrac{\partial f_i}{\partial d_j}.
    \label{eq:B}
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A new look at implicit models}

Formulate the adjustment using $r^Tr$.
Show that the Gauss-Newton method
for non-linear problems
is a particular case of $B = -I$.
Show that the linear problem
is a particular case of $A$ not depending on $p$
and $p^0 = 0$.

Formulate the adjustment with $\delta^T\delta$
and show that the method of Marquardt
is a particular case of this.



\subsection{Regularization}

Add Tikhonov regularization to the combined adjustment derived above.



\subsection{Calculating synthetic data}

Show how to use Newton's root finding method to calculate predicted data from a
given $p$. Only valid when $L = N$.

Show least squares solution for under- and over-determined problems using
$f^tf$.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Examples of implicit models in geophysics}

Give examples here of implicit models.
If only one example


\subsection{Heat flow  at the base of the crust}

\subsection{Love wave dispersion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgments}

We are indebted to the developers and maintainers of the open-source
software without which this work would not have been possible.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
